# -*- coding: utf-8 -*-
"""My Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VKlpE4o9MS3bVNG9IcCtdHIM18pVc-Rn
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler

# --- 1. DATA GENERATION ---
X, y = make_classification(
    n_samples=1000,
    n_features=5,
    n_informative=5,
    n_redundant=0,
    random_state=42
)

# Standardizing features is crucial for gradient descent stability
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 2. LOGISTIC REGRESSION FROM SCRATCH ---
class ScratchLogisticRegression:
    def __init__(self, lr=0.01, iterations=1000):
        self.lr = lr
        self.iterations = iterations
        self.weights = None
        self.bias = None
        self.cost_history = []

    def _sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def _compute_cost(self, y_true, y_pred):
        # Binary Cross-Entropy Cost
        epsilon = 1e-15 # To avoid log(0)
        n = len(y_true)
        cost = -(1/n) * np.sum(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))
        return cost

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.iterations):
            # Linear model + Sigmoid
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self._sigmoid(linear_model)

            # Gradient Calculation
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # Update Rules
            self.weights -= self.lr * dw
            self.bias -= self.lr * db

            self.cost_history.append(self._compute_cost(y, y_predicted))

    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self._sigmoid(linear_model)
        return [1 if i > 0.5 else 0 for i in y_predicted]

# --- 3. TRAIN SCRATCH MODEL ---
scratch_model = ScratchLogisticRegression(lr=0.1, iterations=1000)
scratch_model.fit(X_train, y_train)
scratch_preds = scratch_model.predict(X_test)

# --- 4. TRAIN SCIKIT-LEARN MODEL ---
# Note: penalty=None ensures no regularization for a fair comparison
sk_model = LogisticRegression(penalty=None)
sk_model.fit(X_train, y_train)
sk_preds = sk_model.predict(X_test)

# --- 5. EVALUATION & OUTPUT ---
def print_metrics(name, y_true, y_pred):
    print(f"--- {name} Metrics ---")
    print(f"Accuracy:  {accuracy_score(y_true, y_pred):.4f}")
    print(f"Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"Recall:    {recall_score(y_true, y_pred):.4f}\n")

print(f"Final Cost (Scratch Model): {scratch_model.cost_history[-1]:.6f}\n")
print_metrics("Scratch Implementation", y_test, scratch_preds)
print_metrics("Scikit-Learn (No Penalty)", y_test, sk_preds)

# --- SIMPLE TEXT-BASED DECISION BOUNDARY (Feature 1 vs Feature 2) ---
print("Simple Visual Logic: Feature 1 vs Feature 2 Projections")
print("(Checking if weights favor similar directions)")
print(f"Scratch Weights (first 2): {scratch_model.weights[:2]}")
print(f"Sklearn Weights (first 2): {sk_model.coef_[0][:2]}")